{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<CENTER><img src=\"../../images/ATLASOD.gif\" style=\"width:50%\"></CENTER>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graviton Search! \n",
    "This notebook uses ATLAS Open Data http://opendata.atlas.cern to show you the steps to search for the Graviton yourself!\n",
    "\n",
    "ATLAS Open Data provides open access to proton-proton collision data at the LHC for educational purposes. ATLAS Open Data resources are ideal for high-school, undergraduate and postgraduate students.\n",
    "\n",
    "Notebooks are web applications that allow you to create and share documents that can contain for example:\n",
    "1. live code\n",
    "2. visualisations\n",
    "3. narrative text\n",
    "\n",
    "The idea is that cuts increase the ratio of signal (Graviton) to background ($Z, t\\bar{t}, t\\bar{t}V, ZZ$)\n",
    "\n",
    "This analysis loosely follows the search for a ZZ resonance by ATLAS https://link.springer.com/article/10.1140%2Fepjc%2Fs10052-018-5686-3 (mostly Figure 4)\n",
    "\n",
    "By the end of this notebook you will be able to:\n",
    "1. search for the Graviton yourself!\n",
    "2. know some general principles of a particle physics search\n",
    "\n",
    "Feynman diagram pictures are borrowed from our friends at https://www.particlezoo.net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<CENTER><img src=\"images/feynman_diagrams/Graviton_feynman.png\" style=\"width:40%\"></CENTER>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='contents'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='running'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='setup_computer'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'yaml'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01myaml\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msubprocess\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msys\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'yaml'"
     ]
    }
   ],
   "source": [
    "import yaml\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "environment_file = \"/Users/daniya/Documents/SPQ/environment.yaml\"\n",
    "\n",
    "# Packages you want to install\n",
    "required_packages = ['uproot', 'pandas', 'numpy', 'matplotlib']\n",
    "\n",
    "# Load the environment.yml file\n",
    "with open(environment_file, 'r') as file:\n",
    "    environment_data = yaml.safe_load(file)\n",
    "\n",
    "# Extract dependencies\n",
    "dependencies = environment_data.get('dependencies', [])\n",
    "\n",
    "# Create a list to hold the packages with versions\n",
    "install_packages = []\n",
    "\n",
    "# Find the versions for the required packages\n",
    "for dep in dependencies:\n",
    "    # Check if the dependency is a string (package name)\n",
    "    if isinstance(dep, str):\n",
    "        for package in required_packages:\n",
    "            if dep.startswith(package):\n",
    "                install_packages.append(dep)\n",
    "\n",
    "# Install packages \n",
    "if install_packages:\n",
    "    print(f\"Installing packages: {install_packages}\")\n",
    "    subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"--upgrade\", \"--user\"] + install_packages)\n",
    "else:\n",
    "    print(\"No matching packages found in environment.yml.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to contents](#contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Importing Packages\n",
    "\n",
    "\n",
    "We're going to be using a number of tools to help us:\n",
    "\n",
    "* uproot: lets us read .root files typically used in particle physics into data formats used in python\n",
    "\n",
    "* pandas: lets us store data as dataframes, a format widely used in python\n",
    "\n",
    "* numpy: provides numerical calculations such as histogramming\n",
    "\n",
    "* matplotlib: common tool for making plots, figures, images, visualisations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uproot # for reading .root files\n",
    "import awkward as ak # added to improve how data is stored\n",
    "import pandas as pd # to store data as dataframe\n",
    "import time # to measure time to analyse\n",
    "import math # for mathematical functions such as square root\n",
    "import numpy as np # for numerical calculations such as histogramming\n",
    "import matplotlib.pyplot as plt # for plotting\n",
    "from matplotlib.ticker import AutoMinorLocator,LogLocator,LogFormatterSciNotation # for minor ticks\n",
    "\n",
    "import infofile # local file containing cross-sections, sums of weights, dataset IDs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='fraction'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lumi = 0.5 # fb-1 # data_A only\n",
    "#lumi = 1.9 # fb-1 # data_B only\n",
    "#lumi = 2.9 # fb-1 # data_C only\n",
    "#lumi = 4.7 # fb-1 # data_D only\n",
    "lumi = 10 # fb-1 # data_A,data_B,data_C,data_D\n",
    "\n",
    "fraction = 0.09 # reduce this is you want the code to run quicker\n",
    "                                                                                                                                  \n",
    "#tuple_path = \"Input/4lep/\" # local \n",
    "tuple_path = \"https://atlas-opendata.web.cern.ch/atlas-opendata/samples/2020/4lep/\" # web address\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to contents](#contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Dictionary of samples to process\n",
    "\n",
    "Contains prefixes and filepaths so we can make filestrings to access Open Data ATLAS files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = {\n",
    "\n",
    "    'data': {\n",
    "        'list' : ['data_A','data_B','data_C','data_D']\n",
    "    },\n",
    "    \n",
    "    r'$Z,t\\bar{t}$' : { # Z + ttbar\n",
    "        'list' : ['Zee','Zmumu','ttbar_lep'],\n",
    "        'color' : \"#6b59d3\" # purple\n",
    "    },\n",
    "    \n",
    "    r'$t\\bar{t}V$' : { # ttV\n",
    "        'list' : ['ttW','ttee','ttmumu'], # ttW, ttZ(->ee), ttZ(->mm)\n",
    "        'color' : \"#f0f423\" # yellow\n",
    "    },\n",
    "    \n",
    "    'ZZ' : { # ZZ->llll\n",
    "        'list' : ['llll'],\n",
    "        'color' : \"#ff0000\" # red\n",
    "    },\n",
    "    \n",
    "    'Graviton' : {\n",
    "        'list' : ['RS_G_ZZ_llll_c10_m0500'], # mG = 500 GeV\n",
    "        'color' : \"#baff8d\" # green\n",
    "    },\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Defining Functions\n",
    "\n",
    "\n",
    "Define function to get data from files\n",
    "\n",
    "The datasets used in this notebook have already been filtered to include at least 4 leptons per event, so that processing is quicker."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to contents](#contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define function to calculate:\n",
    "* weight of MC event\n",
    "\n",
    "* cross-section weight\n",
    "\n",
    "* 4-lepton invariant mass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to contents](#contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='changing_cut'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cut_lep_charge(lep_charge):\n",
    "# throw away when sum of lepton charges is not equal to 0\n",
    "# first lepton is [0], 2nd lepton is [1] etc\n",
    "    return lep_charge[0] + lep_charge[1] + lep_charge[2] + lep_charge[3] != 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to contents](#contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(path,sample):\n",
    "    start = time.time() # start the clock\n",
    "    print(\"\\tProcessing: \"+sample) # print which sample is being processed\n",
    "    data_all = pd.DataFrame() # define empty pandas DataFrame to hold all data for this sample\n",
    "    tree = uproot3.open(path)[\"mini\"] # open the tree called mini\n",
    "    numevents = uproot3.numentries(path, \"mini\") # number of events\n",
    "    if 'data' not in sample: xsec_weight = get_xsec_weight(sample) # get cross-section weight\n",
    "    for data in tree.iterate(['lep_pt','lep_eta','lep_phi',\n",
    "                              'lep_E','lep_charge','lep_type', \n",
    "                              # add more variables here if you make cuts on them \n",
    "                              'mcWeight','scaleFactor_PILEUP',\n",
    "                              'scaleFactor_ELE','scaleFactor_MUON',\n",
    "                              'scaleFactor_LepTRIGGER'], # variables to calculate Monte Carlo weight\n",
    "                             outputtype=pd.DataFrame, # choose output type as pandas DataFrame\n",
    "                             entrystop=numevents*fraction): # process up to numevents*fraction\n",
    "\n",
    "        nIn = len(data.index) # number of events in this batch\n",
    "\n",
    "        if 'data' not in sample: # only do this for Monte Carlo simulation files\n",
    "            # multiply all Monte Carlo weights and scale factors together to give total weight\n",
    "            data['totalWeight'] = np.vectorize(calc_weight)(xsec_weight,\n",
    "                                                            data.mcWeight,\n",
    "                                                            data.scaleFactor_PILEUP,\n",
    "                                                            data.scaleFactor_ELE,\n",
    "                                                            data.scaleFactor_MUON,\n",
    "                                                            data.scaleFactor_LepTRIGGER)\n",
    "\n",
    "        # cut on lepton charge using the function cut_lep_charge defined above\n",
    "        fail = data[ np.vectorize(cut_lep_charge)(data.lep_charge) ].index\n",
    "        data.drop(fail, inplace=True)\n",
    "\n",
    "        # cut on lepton type using the function cut_lep_type defined above\n",
    "        fail = data[ np.vectorize(cut_lep_type)(data.lep_type) ].index\n",
    "        data.drop(fail, inplace=True)\n",
    "\n",
    "        # calculation of 4-lepton invariant mass using the function calc_mllll defined above\n",
    "        data['mllll'] = np.vectorize(calc_mllll)(data.lep_pt,data.lep_eta,data.lep_phi,data.lep_E)\n",
    "        \n",
    "        # dataframe contents can be printed at any stage like this\n",
    "        #print(data)\n",
    "\n",
    "        # dataframe column can be printed at any stage like this\n",
    "        #print(data['lep_pt'])\n",
    "\n",
    "        # multiple dataframe columns can be printed at any stage like this\n",
    "        #print(data[['lep_pt','lep_eta']])\n",
    "\n",
    "        nOut = len(data.index) # number of events passing cuts in this batch\n",
    "        data_all = pd.concat([data_all, data], ignore_index=True) # append dataframe from this batch to the dataframe for the whole sample\n",
    "        elapsed = time.time() - start # time taken to process\n",
    "        print(\"\\t\\t nIn: \"+str(nIn)+\",\\t nOut: \\t\"+str(nOut)+\"\\t in \"+str(round(elapsed,1))+\"s\") # events before and after\n",
    "    \n",
    "    return data_all # return dataframe containing events passing all cuts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to contents](#contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing data samples\n",
      "\tProcessing: data_A\n"
     ]
    },
    {
     "ename": "OverflowError",
     "evalue": "Python integer 20112 out of bounds for uint8",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOverflowError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;66;03m# time at start of whole processing\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[43mget_data_from_files\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# process all files\u001b[39;00m\n\u001b[1;32m      3\u001b[0m elapsed \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start \u001b[38;5;66;03m# time after whole processing\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTime taken: \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mround\u001b[39m(elapsed,\u001b[38;5;241m1\u001b[39m))\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ms\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;66;03m# print total time taken to process every file\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[16], line 12\u001b[0m, in \u001b[0;36mget_data_from_files\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m         prefix \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMC/mc_\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;28mstr\u001b[39m(infofile\u001b[38;5;241m.\u001b[39minfos[val][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDSID\u001b[39m\u001b[38;5;124m\"\u001b[39m])\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     11\u001b[0m     fileString \u001b[38;5;241m=\u001b[39m tuple_path\u001b[38;5;241m+\u001b[39mprefix\u001b[38;5;241m+\u001b[39mval\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.4lep.root\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;66;03m# file name to open\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m     temp \u001b[38;5;241m=\u001b[39m \u001b[43mread_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfileString\u001b[49m\u001b[43m,\u001b[49m\u001b[43mval\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# call the function read_file defined below\u001b[39;00m\n\u001b[1;32m     13\u001b[0m     frames\u001b[38;5;241m.\u001b[39mappend(temp) \u001b[38;5;66;03m# append dataframe returned from read_file to list of dataframes\u001b[39;00m\n\u001b[1;32m     14\u001b[0m data[s] \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat(frames) \u001b[38;5;66;03m# dictionary entry is concatenated dataframes\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[21], line 5\u001b[0m, in \u001b[0;36mread_file\u001b[0;34m(path, sample)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124mProcessing: \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m+\u001b[39msample) \u001b[38;5;66;03m# print which sample is being processed\u001b[39;00m\n\u001b[1;32m      4\u001b[0m data_all \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame() \u001b[38;5;66;03m# define empty pandas DataFrame to hold all data for this sample\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m tree \u001b[38;5;241m=\u001b[39m \u001b[43muproot3\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmini\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;66;03m# open the tree called mini\u001b[39;00m\n\u001b[1;32m      6\u001b[0m numevents \u001b[38;5;241m=\u001b[39m uproot3\u001b[38;5;241m.\u001b[39mnumentries(path, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmini\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;66;03m# number of events\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m sample: xsec_weight \u001b[38;5;241m=\u001b[39m get_xsec_weight(sample) \u001b[38;5;66;03m# get cross-section weight\u001b[39;00m\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.10/site-packages/uproot3/rootio.py:61\u001b[0m, in \u001b[0;36mopen\u001b[0;34m(path, localsource, xrootdsource, httpsource, **options)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m xrootd(path, xrootdsource\u001b[38;5;241m=\u001b[39mxrootdsource, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions)\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m _bytesid(parsed\u001b[38;5;241m.\u001b[39mscheme) \u001b[38;5;241m==\u001b[39m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttp\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m _bytesid(parsed\u001b[38;5;241m.\u001b[39mscheme) \u001b[38;5;241m==\u001b[39m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m---> 61\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mhttp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhttpsource\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhttpsource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     64\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mURI scheme not recognized: \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(path))\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.10/site-packages/uproot3/rootio.py:90\u001b[0m, in \u001b[0;36mhttp\u001b[0;34m(path, httpsource, **options)\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     89\u001b[0m     openfcn \u001b[38;5;241m=\u001b[39m httpsource\n\u001b[0;32m---> 90\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mROOTDirectory\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mopenfcn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.10/site-packages/uproot3/rootio.py:154\u001b[0m, in \u001b[0;36mROOTDirectory.read\u001b[0;34m(source, *args, **options)\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m read_streamers \u001b[38;5;129;01mand\u001b[39;00m fSeekInfo \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    153\u001b[0m     streamercontext \u001b[38;5;241m=\u001b[39m ROOTDirectory\u001b[38;5;241m.\u001b[39m_FileContext(source\u001b[38;5;241m.\u001b[39mpath, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m, streamerclasses, uproot3\u001b[38;5;241m.\u001b[39msource\u001b[38;5;241m.\u001b[39mcompressed\u001b[38;5;241m.\u001b[39mCompression(fCompress), tfile)\n\u001b[0;32m--> 154\u001b[0m     streamerkey \u001b[38;5;241m=\u001b[39m \u001b[43mTKey\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mCursor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfSeekInfo\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstreamercontext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    155\u001b[0m     streamerinfos, streamerinfosmap, streamerrules \u001b[38;5;241m=\u001b[39m _readstreamers(streamerkey\u001b[38;5;241m.\u001b[39m_source, streamerkey\u001b[38;5;241m.\u001b[39m_cursor, streamercontext, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    156\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.10/site-packages/uproot3/rootio.py:989\u001b[0m, in \u001b[0;36mROOTObject.read\u001b[0;34m(cls, source, cursor, context, parent)\u001b[0m\n\u001b[1;32m    987\u001b[0m     context \u001b[38;5;241m=\u001b[39m context\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m    988\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__new__\u001b[39m(\u001b[38;5;28mcls\u001b[39m)\n\u001b[0;32m--> 989\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_readinto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcursor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparent\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    990\u001b[0m out\u001b[38;5;241m.\u001b[39m_postprocess(source, cursor, context, parent)\n\u001b[1;32m    991\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.10/site-packages/uproot3/rootio.py:1019\u001b[0m, in \u001b[0;36mTKey._readinto\u001b[0;34m(cls, self, source, cursor, context, parent)\u001b[0m\n\u001b[1;32m   1016\u001b[0m     cursor\u001b[38;5;241m.\u001b[39mindex \u001b[38;5;241m=\u001b[39m start\n\u001b[1;32m   1017\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fNbytes, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fVersion, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fObjlen, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fDatime, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fKeylen, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fCycle, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fSeekKey, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fSeekPdir \u001b[38;5;241m=\u001b[39m cursor\u001b[38;5;241m.\u001b[39mfields(source, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_big)\n\u001b[0;32m-> 1019\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fClassName \u001b[38;5;241m=\u001b[39m \u001b[43mcursor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstring\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1020\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fName \u001b[38;5;241m=\u001b[39m cursor\u001b[38;5;241m.\u001b[39mstring(source)\n\u001b[1;32m   1021\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fTitle \u001b[38;5;241m=\u001b[39m cursor\u001b[38;5;241m.\u001b[39mstring(source)\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.10/site-packages/uproot3/source/cursor.py:74\u001b[0m, in \u001b[0;36mCursor.string\u001b[0;34m(self, source)\u001b[0m\n\u001b[1;32m     72\u001b[0m     length \u001b[38;5;241m=\u001b[39m source\u001b[38;5;241m.\u001b[39mdata(start, stop, numpy\u001b[38;5;241m.\u001b[39mdtype(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m>u4\u001b[39m\u001b[38;5;124m\"\u001b[39m))[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     73\u001b[0m start \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex\n\u001b[0;32m---> 74\u001b[0m stop \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex \u001b[38;5;241m=\u001b[39m \u001b[43mstart\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mlength\u001b[49m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _tobytes(source\u001b[38;5;241m.\u001b[39mdata(start, stop))\n",
      "\u001b[0;31mOverflowError\u001b[0m: Python integer 20112 out of bounds for uint8"
     ]
    }
   ],
   "source": [
    "start = time.time() # time at start of whole processing\n",
    "data = get_data_from_files() # process all files\n",
    "elapsed = time.time() - start # time after whole processing\n",
    "print(\"Time taken: \"+str(round(elapsed,1))+\"s\") # print total time taken to process every file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to contents](#contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Plotting\n",
    "\n",
    "* Define class to display log values\n",
    "\n",
    "* Define function to plot the data \n",
    "\n",
    "* Calls function to plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define class to display 1 and 10 normally\n",
    "class CustomTicker(LogFormatterSciNotation): \n",
    "    def __call__(self, x, pos=None): \n",
    "        if x not in [1,10]: #Â not 1 or 10\n",
    "            return LogFormatterSciNotation.__call__(self,x, pos=None)\n",
    "        else: # 1 or 10\n",
    "            return \"{x:g}\".format(x=x) # standard notation\n",
    "        \n",
    "def plot_data(data):\n",
    "\n",
    "    xmin = 130 # GeV\n",
    "    xmax = 1230 # GeV\n",
    "    step_size = 55 # GeV\n",
    "\n",
    "    bin_edges = np.arange(start=xmin, # The interval includes this value\n",
    "                     stop=xmax+step_size, # The interval doesn't include this value\n",
    "                     step=step_size ) # Spacing between values\n",
    "    bin_centres = np.arange(start=xmin+step_size/2, # The interval includes this value\n",
    "                            stop=xmax+step_size/2, # The interval doesn't include this value\n",
    "                            step=step_size ) # Spacing between values\n",
    "\n",
    "    data_x,_ = np.histogram(data['data']['mllll'], \n",
    "                            bins=bin_edges ) # histogram the data\n",
    "    data_x_errors = np.sqrt( data_x ) # statistical error on the data\n",
    "\n",
    "    signal_x = data['Graviton']['mllll'] # histogram the signal\n",
    "    signal_weights = data['Graviton'].totalWeight # get the weights of the signal events\n",
    "    signal_color = samples['Graviton']['color'] # get the colour for the signal bar\n",
    "\n",
    "    mc_x = [] # define list to hold the Monte Carlo histogram entries\n",
    "    mc_weights = [] # define list to hold the Monte Carlo weights\n",
    "    mc_colors = [] # define list to hold the colors of the Monte Carlo bars\n",
    "    mc_labels = [] # define list to hold the legend labels of the Monte Carlo bars\n",
    "\n",
    "    for s in samples: # loop over samples\n",
    "        if s not in ['data', 'Graviton']: # if not data nor signal\n",
    "            mc_x.append( data[s]['mllll'] ) # append to the list of Monte Carlo histogram entries\n",
    "            mc_weights.append( data[s].totalWeight ) # append to the list of Monte Carlo weights\n",
    "            mc_colors.append( samples[s]['color'] ) # append to the list of Monte Carlo bar colors\n",
    "            mc_labels.append( s ) # append to the list of Monte Carlo legend labels\n",
    "    \n",
    "\n",
    "\n",
    "    # *************\n",
    "    # Main plot \n",
    "    # *************\n",
    "    main_axes = plt.gca() # get current axes\n",
    "    \n",
    "    # plot the data points\n",
    "    main_axes.errorbar(x=bin_centres, y=data_x, yerr=data_x_errors,\n",
    "                       fmt='ko', # 'k' means black and 'o' is for circles \n",
    "                       label='Data') \n",
    "    \n",
    "    # plot the Monte Carlo bars\n",
    "    mc_heights = main_axes.hist(mc_x, bins=bin_edges, \n",
    "                                weights=mc_weights, stacked=True, \n",
    "                                color=mc_colors, label=mc_labels )\n",
    "    \n",
    "    mc_x_tot = mc_heights[0][-1] # stacked background MC y-axis value\n",
    "    \n",
    "    # calculate MC statistical uncertainty: sqrt(sum w^2)\n",
    "    mc_x_err = np.sqrt(np.histogram(np.hstack(mc_x), bins=bin_edges, weights=np.hstack(mc_weights)**2)[0])\n",
    "    \n",
    "    # plot the signal bar\n",
    "    main_axes.hist(signal_x, bins=bin_edges, bottom=mc_x_tot, \n",
    "                   weights=signal_weights, color=signal_color,\n",
    "                   label='Graviton')\n",
    "    \n",
    "    # plot the statistical uncertainty\n",
    "    main_axes.bar(bin_centres, # x\n",
    "                  2*mc_x_err, # heights\n",
    "                  alpha=0.5, # half transparency\n",
    "                  bottom=mc_x_tot-mc_x_err, color='none', \n",
    "                  hatch=\"////\", width=step_size, label='Stat. Unc.' )\n",
    "\n",
    "    # set the x-limit of the main axes\n",
    "    main_axes.set_xlim( left=xmin, right=xmax ) \n",
    "    \n",
    "    # separation of x axis minor ticks\n",
    "    main_axes.xaxis.set_minor_locator( AutoMinorLocator() ) \n",
    "    \n",
    "    # set the axis tick parameters for the main axes\n",
    "    main_axes.tick_params(which='both', # ticks on both x and y axes\n",
    "                          direction='in', # Put ticks inside and outside the axes\n",
    "                          top=True, # draw ticks on the top axis\n",
    "                          right=True ) # draw ticks on right axis\n",
    "    \n",
    "    # x-axis label\n",
    "    main_axes.set_xlabel(r'4-lepton invariant mass $\\mathrm{m_{4l}}$ [GeV]',\n",
    "                        fontsize=13, x=1, horizontalalignment='right' )\n",
    "    \n",
    "    # write y-axis label for main axes\n",
    "    main_axes.set_ylabel('Events / '+str(step_size)+' GeV',\n",
    "                         y=1, horizontalalignment='right') \n",
    "    \n",
    "    # add minor ticks on y-axis for main axes\n",
    "    main_axes.yaxis.set_minor_locator( AutoMinorLocator() ) \n",
    "    \n",
    "    main_axes.set_yscale('log') # set y-scale\n",
    "    smallest_contribution = mc_heights[0][0] # get smallest contribution\n",
    "    smallest_contribution.sort() # sort smallest contribution\n",
    "    bottom = np.amax(data_x)/1000 # set bottom limit on y-axis\n",
    "    top = np.amax(data_x)*100 # set top limit on y-axis\n",
    "    main_axes.set_ylim( bottom=bottom, top=top ) # y-axis limits\n",
    "    main_axes.yaxis.set_major_formatter( CustomTicker() ) \n",
    "    locmin = LogLocator(base=10.0, # log base 10\n",
    "                        subs=(0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9) ) # minor tick every 0.1 \n",
    "    main_axes.yaxis.set_minor_locator( locmin ) # set minor ticks\n",
    "\n",
    "    # Add text 'ATLAS Open Data' on plot\n",
    "    plt.text(0.05, # x\n",
    "             0.93, # y\n",
    "             'ATLAS Open Data', # text\n",
    "             transform=main_axes.transAxes, # coordinate system used is that of main_axes\n",
    "             fontsize=13 ) \n",
    "    \n",
    "    # Add text 'for education' on plot\n",
    "    plt.text(0.05, # x\n",
    "             0.88, # y\n",
    "             'for education', # text\n",
    "             transform=main_axes.transAxes, # coordinate system used is that of main_axes\n",
    "             style='italic',\n",
    "             fontsize=8 ) \n",
    "    \n",
    "    # Add energy and luminosity\n",
    "    lumi_used = str(round(lumi*fraction,2)) # luminosity to write on the plot\n",
    "    plt.text(0.05, # x\n",
    "             0.82, # y\n",
    "             '$\\sqrt{s}$=13 TeV, '+lumi_used+' fb$^{-1}$', # text\n",
    "             transform=main_axes.transAxes ) # coordinate system used is that of main_axes\n",
    "    \n",
    "    # Add a label for the analysis carried out\n",
    "    plt.text(0.05, # x\n",
    "             0.75, # y\n",
    "             r'$G \\rightarrow ZZ \\rightarrow l^+l^-l^+l^-$', # text \n",
    "             transform=main_axes.transAxes ) # coordinate system used is that of main_axes\n",
    "\n",
    "    # draw the legend\n",
    "    main_axes.legend(ncol=2, # 2 columns\n",
    "                     frameon=False ) # no box around the legend\n",
    "    \n",
    "    return\n",
    "\n",
    "plot_data(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='going_further'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What can you do to explore this analysis?\n",
    "\n",
    "* Increase the fraction of data used in '[Lumi, fraction, file path](#fraction)'\n",
    "* Check how many events are being thrown away by each cut in '[Applying a cut](#applying_cut)'\n",
    "* Add more cuts from the [ATLAS paper searching for ZZ resonances](https://link.springer.com/article/10.1140%2Fepjc%2Fs10052-018-5686-3#Sec5) in '[Changing a cut](#changing_cut)' and '[Applying a cut](#applying_cut)'\n",
    "* Add a plot to show the ratio between Data and MC other than Graviton like [Figure 4 of the ATLAS paper searching for ZZ resonances](https://link.springer.com/article/10.1140%2Fepjc%2Fs10052-018-5686-3#Fig4)\n",
    "* Get the estimated numbers of events, similar to [Table 4 of the ATLAS paper searching for ZZ resonances](https://link.springer.com/article/10.1140/epjc/s10052-018-5686-3/tables/4)\n",
    "* Split the analysis into ggF and VBF, like [Section 5 of the ATLAS paper searching for ZZ resonances](https://link.springer.com/article/10.1140%2Fepjc%2Fs10052-018-5686-3#Sec5)\n",
    "* Your idea!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3.11.2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
